{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PROCESSED_DATA_DIR = \"../../data/processed data\"\n",
    "MERGED_DATA_DIR = \"../../data/merged data\"\n",
    "\n",
    "SUB_FOLDER = \"2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Common Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, \"census_tract_clusters.csv\"))\n",
    "# public_transportation = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, \"cleaned_public_transportation_data.csv\"))\n",
    "cta_L = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, \"cleaned_cta_L_data.csv\"))\n",
    "cta_bus = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, \"cleaned_cta_bus_data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnp_trips = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, SUB_FOLDER, \"cleaned_tnp_data.csv\"))\n",
    "tnp_trips = tnp_trips[(tnp_trips[\"trip_start_time_of_day_f\"] >= 6.0) & (tnp_trips[\"trip_start_time_of_day_f\"] < 22.0)]\n",
    "weather = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, SUB_FOLDER, \"cleaned_weather_data.csv\"))\n",
    "taxi_trips = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, SUB_FOLDER, \"cleaned_taxi_data.csv\"))\n",
    "\n",
    "taxi_trips = (\n",
    "    taxi_trips\n",
    "    .groupby(['trip_start_timestamp', 'pickup_census_tract', 'dropoff_census_tract'])\n",
    "    .size()\n",
    "    .reset_index(name='taxi')\n",
    ")\n",
    "\n",
    "tnp_trips[\"pickup_census_tract\"] = tnp_trips[\"pickup_census_tract\"].astype(int)\n",
    "acs[\"GEOID\"] = acs[\"GEOID\"].astype(int)\n",
    "\n",
    "# tnp_trips['trip_start_date'] = pd.to_datetime(tnp_trips['trip_start_date']).dt.date\n",
    "# cta_L['date'] = pd.to_datetime(cta_L['date']).dt.date\n",
    "# cta_bus['date'] = pd.to_datetime(cta_bus['date']).dt.date\n",
    "\n",
    "merged_tnp_data = tnp_trips.merge(acs, left_on=\"pickup_census_tract\", right_on=\"GEOID\", how=\"left\")\n",
    "merged_tnp_data = merged_tnp_data.merge(weather, left_on=\"trip_start_date\", right_on=\"Date\", how=\"left\")\n",
    "# merged_tnp_data = merged_tnp_data.merge(public_transportation, left_on=\"trip_start_date\", right_on=\"service_date\", how=\"left\")\n",
    "merged_tnp_data = merged_tnp_data.merge(cta_L, left_on=[\"pickup_census_tract\", \"trip_start_date\"], right_on=[\"GEOID10\", \"date\"], how=\"left\")\n",
    "merged_tnp_data = merged_tnp_data.merge(cta_bus, left_on=[\"pickup_census_tract\", \"trip_start_date\"], right_on=[\"GEOID10\", \"date\"], how=\"left\")\n",
    "merged_tnp_data = merged_tnp_data.merge(\n",
    "    taxi_trips,\n",
    "    left_on=['trip_start_timestamp', 'pickup_census_tract', 'dropoff_census_tract'],\n",
    "    right_on=['trip_start_timestamp', 'pickup_census_tract', 'dropoff_census_tract'],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {\n",
    "    \"MEDINC\": \"DP03_0062E\",          # Median Household Income\n",
    "    \"INCPERCAP\": \"DP03_0088E\",       # Per Capita Income\n",
    "    \"INC_LT10K\": \"DP03_0052E\",       # Income < $10k\n",
    "    \"INC_10_15K\": \"DP03_0053E\",\n",
    "    \"INC_15_25K\": \"DP03_0054E\",\n",
    "    \"INC_25_35K\": \"DP03_0055E\",\n",
    "    \"INC_35_50K\": \"DP03_0056E\",\n",
    "    \"INC_50_75K\": \"DP03_0057E\",\n",
    "    \"INC_75_100K\": \"DP03_0058E\",\n",
    "    \"INC_100_150K\": \"DP03_0059E\",\n",
    "    \"INC_150_200K\": \"DP03_0060E\",\n",
    "    \"INC_GT200K\": \"DP03_0061E\",\n",
    "    \"POVERTY_RATE\": \"DP03_0119PE\",   # % Below Poverty Line\n",
    "    \"SNAP\": \"DP03_0074E\",            # Households with Food Stamps/SNAP\n",
    "    \"SS_INCOME\": \"DP03_0066E\",       # Households with Social Security Income\n",
    "}\n",
    "\n",
    "merged_tnp_data = merged_tnp_data.dropna(subset=list(VARIABLES.keys()))\n",
    "\n",
    "merged_tnp_data[\"day_type\"] = merged_tnp_data[\"daytype\"]\n",
    "merged_tnp_data = merged_tnp_data[merged_tnp_data[\"day_type\"] == 'W']\n",
    "\n",
    "merged_tnp_data['taxi'] = merged_tnp_data['taxi'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicate Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tnp_data = merged_tnp_data.drop(columns=[\n",
    "                                        \"trip_id\", \"trip_end_timestamp\",\n",
    "                                        \"pickup_centroid_latitude\", \"pickup_centroid_longitude\",\n",
    "                                        \"dropoff_centroid_latitude\", \"dropoff_centroid_longitude\",\n",
    "                                        \"trip_end_date\",\n",
    "                                        \"nonworkday\",\n",
    "                                        \"GEOID\",\n",
    "                                        \"day_type\",\n",
    "                                        \"shared_trip_authorized\", \"trips_pooled\",\n",
    "                                        \"pickup_centroid_location\", \"dropoff_centroid_location\",\n",
    "                                        \"Date\", \"Station\",\n",
    "                                        # \"service_date\",\n",
    "                                        \"WSI\",\n",
    "                                        \"GEOID10_x\", \"date_x\", \"GEOID10_y\", \"date_y\",\n",
    "                                        ])\n",
    "\n",
    "merged_tnp_data['total_rides'] = merged_tnp_data['rides'] + merged_tnp_data['daily_bus_rides']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_area(pickup, dropoff):\n",
    "    loop_area = 32\n",
    "    special_areas = [76, 8, 56, 33]  # O'Hare, Near North Side, Garfield Ridge, Near South Side\n",
    "\n",
    "    if loop_area in [pickup, dropoff]:\n",
    "        return 1  # Loop\n",
    "    elif pickup in special_areas or dropoff in special_areas:\n",
    "        return 2  # Other target zones\n",
    "    else:\n",
    "        return 0  # Other\n",
    "\n",
    "# 确保区域编号为整数\n",
    "merged_tnp_data[\"pickup_community_area\"] = merged_tnp_data[\"pickup_community_area\"].astype(int)\n",
    "merged_tnp_data[\"dropoff_community_area\"] = merged_tnp_data[\"dropoff_community_area\"].astype(int)\n",
    "\n",
    "# 应用分类函数\n",
    "merged_tnp_data[\"area_type\"] = merged_tnp_data.apply(\n",
    "    lambda row: classify_area(row[\"pickup_community_area\"], row[\"dropoff_community_area\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 添加 day_of_week（如原始代码）\n",
    "merged_tnp_data[\"trip_start_timestamp\"] = pd.to_datetime(merged_tnp_data[\"trip_start_timestamp\"])\n",
    "merged_tnp_data[\"day_of_week\"] = merged_tnp_data[\"trip_start_timestamp\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否存在 NaN: False\n",
      "NaN 总数: 0\n",
      "\n",
      " total_rides 统计信息：\n",
      "count    1.705779e+06\n",
      "mean     5.263564e+04\n",
      "std      7.371695e+04\n",
      "min      6.163831e+02\n",
      "25%      8.951843e+03\n",
      "50%      1.489467e+04\n",
      "75%      4.943864e+04\n",
      "max      2.202542e+05\n",
      "Name: total_rides, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 检查是否存在 NaN\n",
    "has_nan = merged_tnp_data[\"total_rides\"].isna().any()\n",
    "nan_count = merged_tnp_data[\"total_rides\"].isna().sum()\n",
    "\n",
    "print(f\"是否存在 NaN: {has_nan}\")\n",
    "print(f\"NaN 总数: {nan_count}\")\n",
    "\n",
    "# 查看最大值、最小值、均值等\n",
    "print(\"\\n total_rides 统计信息：\")\n",
    "print(merged_tnp_data[\"total_rides\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tnp_data.to_csv(os.path.join(MERGED_DATA_DIR, SUB_FOLDER, \"merged_tnp_data.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
